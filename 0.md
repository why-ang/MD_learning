2.想没想过文本生成用强化学习怎么做？

3.Wordvec的原理讲一下，画出CBOW和Skip-gram的模型图

4.Word2vec用的两个Trick是什么？为什么要做层次Softmax，负采样是怎么实现的？

5.讲讲Glove的原理，它和Word2vec有什么区别？Fasttext说一下

6.画LSTM模型图，写LSTM的公式

7.讲讲CNN，它和LSTM有什么区别

8.Transformer介绍一下

9.画一下ELMo的模型图，讲一下ELMo的原理，为什么它能解决词歧义的问题？

10.画Bert的模型图，讲原理，预训练的过程。Bert输入是由哪些组成的？Bert相比于ELMo有什么优点？它是怎么用作下游任务的？

11.Attention机制的原理，常用的Attention计算相似度方式有哪些，写一下公式。

12.CRF的原理

13.有分布式训练神经网络的经验吗？多卡跑模型的命令是什么？

14.代码题：数组中索引K前面是有序的，K之后也是有序的，调整使得整个数组有序，要求空间复杂度O(1)


2.聊项目，解决了什么问题？你们团队是怎么分工的，有几个人，你主要负责什么？

3.围绕项目一顿怼。。。。

4.你的评价指标是什么？讲讲ROUGE和BLEU

5.写出Wordvec在负采样下的目标函数

这个没写上来。。。。

6.又抠了一便Transformer，为什么Transformer要比LSTM的效果好？你的工作为什么用GRU没用Transformer？

7.传统机器学习方法了解吗？问了问SVM和GBDT

8.代码题，有一个M*N的矩阵，每一格有一个正整数，将>=K的连续格子聚类到一起


2.你这个项目解决了什么问题，有什么意义？为什么说它可以提高词向量的可解释性？

3.讲讲Bert为什么效果好

4.智力题：ABCD乘以9等于DCBA，那么ABCD各等于几？

5.代码题：有1,2,5,10,20,50的纸币，求凑到100元一共有多少种方法
