
##### 论文阅读：
 `Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions`
 
 
   当前的多数Image caption模型缺少可控性和可解释性，这使得它与人类智能不同，因为人类能够选择各种描述图片的角度，并根据手头上的任务选择最合理的描述。本文提出了Show,Control and Tell的方法，可以根据控制信号（一个序列或者一个图像的区域集合）来进行多样的描述。
 
主要贡献：
* 提出了一种新的图像字幕框架，可以从外部控制，并且可以明确基于一组图像区域生成描述。
* 模型通过预测名词块的序列来明确地考虑句子的层次结构。此外，它考虑了视觉和文本词之间的区别，在词级提供额外的指导。 
      
方法：

   在描述图像的情况下，我们可以区分视觉词(图像中出现的对象)和文本词(图像中未直接体现的词)。分析词在句法上的依赖性，词可以组织成树的形状，在句法依赖树中，名词和它的修饰词组在一起，形成名词块。一个名词块就基于图片中的一个区域，且图像也可以分成多个区域，每个区域对应一个或多个名词块，描述图像的方式由不同的块序列决定。句法依赖树由spaCy工具包提取，图像区域由Faster RCNN提取。
   
   
   给定图像 `I` 和区域集 `R =(r_0,r_1,...,r_N)` 的有序序列，模型的目标是生成句子 `y =(y_0,y_1,...,y_T)` 它能够描述 `R` 中所有的区域，同时保证语言的流畅性。区域集合`R` 为控制信号，模型预测两个输出分布，分别是生成单词的概率：`p（y_t | R,I;θ)`，和块切换的概率：`p（g_t | R,I;θ）`。训练时，`r_t`和`w_t`是对应于时间t的输入的区域和单词;测试时`r_t`根据块位移门`g_t` 决定是否切换到下一个位置。当块位移门预测块结束，就将生成的过程和控制信号`R`连接起来，当`r_t`被选中后，为此，作者建立了自适应注意机制，来在这个区域中区分视觉词和文本词。
   
生成结果：
在多样性和描述评价指标上达到了Flick30k和COCO的SoTA



`Intention Oriented Image Captions with Guiding Objects`


虽然现有的图像描述模型可以生成很好的结果，但很难保证我们关心的对象包含在生成的描述中，例如一些在图像中不明显的对象。尽管我们能检测出所有目标，但是我们不能强制语言模型描述我们关心的目标，这篇论文，作者提出了一种新的方法CGO，可以确保用户选择的指导对象包含在流畅的描述中。所选对象可以是从图像中检测到的任何对象，即使它在训练数据中是不可见的。

作者结合两个LSTM来生成句子中目标词的左部分和右部分。在这个过程中，重要的是两个序列的内容要一致。在CGO中，每个LSTM对序列的一部分进行编码，然后根据编码的序列和图像的视觉特征生成一个序列。通过向LSTM提供不同的目标词，从而为图像生成更全面、更多样化的描述。


方法 ：
 给定一张图片和目标词，CGO能够将选择的目标融合到生成的句子中,`y = (y_1,..., y_k−1, y_k, y_k+1,...,y_T )`, `y_k`为目标词，在生成过程中，两个LSTM生成目标两侧的文本，一个记为LSTM-L，另一个记为LSTM-R, 模型： θ^∗ = argmax p(y^∗_left|I, y_k, θ)p(y^∗_right|I, y_k, θ)
 
 LSTM-L:  由于生成序列可能包含多个同类型的对象， 在输入中添加了对象序列`S`： `p(y_t|y_t+1, ..., y_k, S) = LSTML(f_t, x_t, h_t+1)` ，`S`在`y_k`之前进行编码 ，这些对象标签被认为是在右边的序列中出现的。训练时`S`从`y_k`的右边提取。
 
 LSTM-R：从LSTM-L中获取左侧部分序列，将该序列作为输入，完成句子的右边部分的生成。
 
 





