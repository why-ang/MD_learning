
##### 论文阅读：
 `Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions`
 
 
   当前的多数Image caption模型缺少可控性和可解释性，这使得它与人类智能不同，因为人类能够选择各种描述图片的角度，并根据手头上的任务选择最合理的描述。本文提出了Show,Control and Tell的方法，可以根据控制信号（一个序列或者一个图像的区域集合）来进行多样的描述。
 
主要贡献：
* 提出了一种新的图像字幕框架，可以从外部控制，并且可以明确基于一组图像区域生成描述。\ 
* 模型通过预测名词块的序列来明确地考虑句子的层次结构。此外，它考虑了视觉和文本词之间的区别，从而在词级提供额外的基础。 
          
   在描述图像的情况下，我们可以区分视觉词(图像中出现的对象)和文本词(图像中未直接体现的词)。分析词在句法上的依赖性，词可以组织成树的形状，在句法依赖树中，名词和它的修饰词组在一起，形成名词块。一个名词块就基于图片中的一个区域，且图像也可以分成多个区域，每个区域对应一个或多个名词块，描述图像的方式由不同的块序列决定。
   
   方法：
   给定图像 `I` 和区域集 `R =(r_0,r_1,...,r_N)` 的有序序列，模型的目标是生成句子 `y =(y_0,y_1,...,y_T)` 它能够描述 `R` 中所有的区域，同时保证语言的流畅性。区域集合`R` 为控制信号，模型预测两个输出分布，分别是生成单词的概率：`p（y_t | R,I;θ)`，和块切换的概率：`p（g_t | R,I;θ）`。训练时，`r_t`和`w_t`是对应于时间t的地面实况区域和单词;测试时`r_t`根据块位移门`g_t` 决定是否切换到下一个位置。当块位移门预测块结束，就将生成的过程和控制信号`R`连接起来，当`r_t`被选中后，为此，作者建立了自适应注意机制，来在这个区域中区分视觉词和文本词。
   
生成结果：
* 在多样性和描述评价指标上达到了Flick30k和COCO的SoTA




